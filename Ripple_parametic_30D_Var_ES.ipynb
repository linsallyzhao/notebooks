{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lin/.pyenv/versions/3.6.3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "/home/lin/.pyenv/versions/3.6.3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/lin/.pyenv/versions/3.6.3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import scipy.stats as stats\n",
    "import scipy.integrate as integrate\n",
    "import statsmodels.graphics.gofplots as sgg\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.neighbors import KernelDensity\n",
    "%matplotlib inline\n",
    "#%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1618.000000\n",
       "mean        0.003731\n",
       "std         0.079982\n",
       "min        -0.616273\n",
       "25%        -0.021178\n",
       "50%        -0.002303\n",
       "75%         0.021177\n",
       "max         1.027356\n",
       "Name: Close, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = pd.read_csv('/home/lin/教材/Financial_data/data/Ripple.csv')\n",
    "my_data['Date']=pd.to_datetime(my_data['Date'])\n",
    "my_data.set_index('Date', inplace = True)\n",
    "close = np.flip(my_data['Close'], 0)\n",
    "volume = my_data['Volume'].dropna()\n",
    "logR = np.log(close).diff()\n",
    "logR.drop(logR.index[0], inplace = True)\n",
    "logR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_obs = logR.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_test = int(180)\n",
    "alpha_one = 0.95\n",
    "alpha_two = 0.99\n",
    "alpha_es = 0.975\n",
    "report_size = total_obs - start_test\n",
    "norm_var_ones = np.zeros(report_size)\n",
    "norm_var_twos = np.zeros(report_size)\n",
    "norm_ES = np.zeros(report_size)\n",
    "t_var_ones = np.zeros(report_size)\n",
    "t_var_twos = np.zeros(report_size)\n",
    "t_ES = np.zeros(report_size)\n",
    "HS_var_ones = np.zeros(report_size)\n",
    "HS_var_twos = np.zeros(report_size)\n",
    "HS_ES = np.zeros(report_size)\n",
    "\n",
    "\n",
    "for i in range(start_test, logR.count()):\n",
    "    training_set = logR[i - start_test : i]\n",
    "    x_ticks = np.linspace(min(training_set), max(abs(training_set)),2000)\n",
    "    hs_prices = training_set.sort_values()\n",
    "    negaR = training_set[training_set <= 0].dropna()\n",
    "    sortedNegaR = (-negaR).sort_values()\n",
    "    mirrorNegative = sortedNegaR.append(-sortedNegaR)\n",
    "    mu_log = np.mean(training_set)\n",
    "    sigma_log = np.std(training_set)\n",
    "    nega_t_para = stats.t.fit(mirrorNegative)\n",
    "    norm_var_ones[i - start_test] = -(sigma_log * stats.norm.ppf(alpha_one) - mu_log)\n",
    "    norm_var_twos[i - start_test] = -(sigma_log * stats.norm.ppf(alpha_two) - mu_log)\n",
    "    norm_ES[i - start_test]  = -sigma_log * stats.norm.pdf(stats.norm.ppf(alpha_es))/(1-alpha_es)- mu_log\n",
    "    t_var_ones[i - start_test] = -stats.t.ppf(alpha_one, df = nega_t_para[0] , loc = nega_t_para[1], scale = nega_t_para[2])\n",
    "    t_var_twos[i - start_test] = -stats.t.ppf(alpha_two, df = nega_t_para[0] , loc = nega_t_para[1], scale = nega_t_para[2])\n",
    "    t_es_quantile = stats.t.ppf(alpha_es, df = nega_t_para[0] , loc = nega_t_para[1], scale = nega_t_para[2])\n",
    "    domain_t = x_ticks[x_ticks <= t_es_quantile]\n",
    "    pdf_t = stats.t.pdf(domain_t, df = nega_t_para[0] , loc = nega_t_para[1], scale = nega_t_para[2])\n",
    "    to_integral = pdf_t * domain_t\n",
    "    t_ES[i - start_test] = integrate.trapz(to_integral, domain_t, dx = 0.01) / (1 - alpha_es)\n",
    "    \n",
    "    ind_var_one = int( hs_prices.size * (1-alpha_one))-1\n",
    "    ind_var_two = int( hs_prices.size * (1-alpha_two))-1\n",
    "    ind_es = int( hs_prices.size * (1-alpha_es))-1\n",
    "    HS_var_ones[i - start_test] = hs_prices.iloc[ind_var_one]\n",
    "    HS_var_twos[i - start_test] = hs_prices.iloc[ind_var_two]\n",
    "    HS_ES[i - start_test] = hs_prices.iloc[:ind_es+1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f41adac5518>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "plt.plot(logR[start_test:].values, 'r',  linewidth = 0.5)\n",
    "plt.plot(norm_var_ones, 'b')\n",
    "plt.plot(norm_var_twos, 'g')\n",
    "plt.plot(norm_ES, 'm')\n",
    "plt.plot(t_var_ones, '--b')\n",
    "plt.plot(t_var_twos, '--g')\n",
    "plt.plot(t_ES, '--m')\n",
    "plt.title('180 days training window parametric estimation of risk measures')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('log return')\n",
    "plt.legend(['log return', 'normal var 95%', 'normal var 99%', 'normal ES 97.5%', 't var 95%', 't var 99%', 't ES 97.5%' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It make sence to use shorter training window for ripple. The absolut values\n",
    "#of extreme moves in ripple are much larger\n",
    "#than that of the common moves. And big movements cluster which means as \n",
    "#long as you observe a big move you need to quickly prepare to deal with more. \n",
    "#Using long trainning windows means the estimated distribution can not react\n",
    "#fast enough to the new change because a few new data point will ont change \n",
    "#much of the whole distribution. One may argur that with long training window\n",
    "#distribution will remember the last time of big movement but I believe there\n",
    "#there is no gurrantee how long it will be between two group of big movement\n",
    "#and it is more reliable to get information from the new change in the market\n",
    "#also, even the distributino remember the big movements, the information will\n",
    "#be diluted by many more small size movement.\n",
    "#However, short window makes the risk measures change a lot during time,\n",
    "#especially for the historical estimation. Because if the window is too small\n",
    "#the historical var might be the lowest return in the whole training window\n",
    "#and as long as that lowest observation moves out, the estmation will change\n",
    "#damatically.\n",
    "#With short trainning window, the ES is smoother and might be higher than \n",
    "#99% var because it is a mean of a few extreme values while 99% var might be\n",
    "#the lowest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f41ad8f6160>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_test = int(350)\n",
    "report_size = total_obs - start_test\n",
    "\n",
    "HS_var_ones = np.zeros(report_size)\n",
    "HS_var_twos = np.zeros(report_size)\n",
    "HS_ES = np.zeros(report_size)\n",
    "for i in range(start_test, logR.count()):\n",
    "    training_set = logR[i - start_test : i]\n",
    "    x_ticks = np.linspace(min(training_set), max(abs(training_set)),2000)\n",
    "    hs_prices = training_set.sort_values()\n",
    "    ind_var_one = int( hs_prices.size * (1-alpha_one))-1\n",
    "    ind_var_two = int( hs_prices.size * (1-alpha_two))-1\n",
    "    ind_es = int( hs_prices.size * (1-alpha_es))-1\n",
    "    HS_var_ones[i - start_test] = hs_prices.iloc[ind_var_one]\n",
    "    HS_var_twos[i - start_test] = hs_prices.iloc[ind_var_two]\n",
    "    HS_ES[i - start_test] = hs_prices.iloc[:ind_es+1].mean()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(logR[start_test:].values, 'r', linewidth = 0.5)\n",
    "plt.plot(HS_var_ones, 'b')\n",
    "plt.plot(HS_var_twos, 'g')\n",
    "plt.plot(HS_ES, 'm')\n",
    "plt.title('350 days training window historical estimation of risk measures')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('log return')\n",
    "plt.legend(['log return', 'HS 95%', 'HS 99%', 'HS ES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
