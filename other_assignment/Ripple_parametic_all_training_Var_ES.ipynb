{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lin/.pyenv/versions/3.6.3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "/home/lin/.pyenv/versions/3.6.3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/lin/.pyenv/versions/3.6.3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import scipy.stats as stats\n",
    "import scipy.integrate as integrate\n",
    "import statsmodels.graphics.gofplots as sgg\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.neighbors import KernelDensity\n",
    "%matplotlib inline\n",
    "#%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1618.000000\n",
       "mean        0.003731\n",
       "std         0.079982\n",
       "min        -0.616273\n",
       "25%        -0.021178\n",
       "50%        -0.002303\n",
       "75%         0.021177\n",
       "max         1.027356\n",
       "Name: Close, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = pd.read_csv('/home/lin/教材/Financial_data/data/Ripple.csv')\n",
    "my_data['Date']=pd.to_datetime(my_data['Date'])\n",
    "my_data.set_index('Date', inplace = True)\n",
    "close = np.flip(my_data['Close'], 0)\n",
    "volume = my_data['Volume'].dropna()\n",
    "logR = np.log(close).diff()\n",
    "logR.drop(logR.index[0], inplace = True)\n",
    "logR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1618"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logR.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_test = int(logR.count() - 252)\n",
    "alpha_one = 0.95\n",
    "alpha_two = 0.99\n",
    "alpha_es = 0.975\n",
    "norm_var_ones = np.zeros(252)\n",
    "norm_var_twos = np.zeros(252)\n",
    "norm_ES = np.zeros(252)\n",
    "t_var_ones = np.zeros(252)\n",
    "t_var_twos = np.zeros(252)\n",
    "t_ES = np.zeros(252)\n",
    "HS_var_ones = np.zeros(252)\n",
    "HS_var_twos = np.zeros(252)\n",
    "HS_ES = np.zeros(252)\n",
    "\n",
    "for i in range(start_test, logR.count()):\n",
    "    training_set = logR[: i]\n",
    "    x_ticks = np.linspace(min(training_set), max(abs(training_set)),2000)\n",
    "    hs_prices = training_set.sort_values()\n",
    "    negaR = training_set[training_set <= 0].dropna()\n",
    "    sortedNegaR = (-negaR).sort_values()\n",
    "    mirrorNegative = sortedNegaR.append(-sortedNegaR)\n",
    "    mu_log = np.mean(training_set)\n",
    "    sigma_log = np.std(training_set)\n",
    "    nega_t_para = stats.t.fit(mirrorNegative)\n",
    "    \n",
    "    norm_var_ones[i - start_test] = -(sigma_log * stats.norm.ppf(alpha_one) - mu_log)\n",
    "    norm_var_twos[i - start_test] = -(sigma_log * stats.norm.ppf(alpha_two) - mu_log)\n",
    "    norm_ES[i - start_test]  = -sigma_log * stats.norm.pdf(stats.norm.ppf(alpha_es))/(1-alpha_es)- mu_log\n",
    "    \n",
    "    t_var_ones[i - start_test] = -stats.t.ppf(alpha_one, df = nega_t_para[0] , loc = nega_t_para[1], scale = nega_t_para[2])\n",
    "    t_var_twos[i - start_test] = -stats.t.ppf(alpha_two, df = nega_t_para[0] , loc = nega_t_para[1], scale = nega_t_para[2])\n",
    "    \n",
    "    t_es_quantile = stats.t.ppf(alpha_es, df = nega_t_para[0] , loc = nega_t_para[1], scale = nega_t_para[2])\n",
    "    domain_t = x_ticks[x_ticks <= t_es_quantile]\n",
    "    pdf_t = stats.t.pdf(domain_t, df = nega_t_para[0] , loc = nega_t_para[1], scale = nega_t_para[2])\n",
    "    to_integral = pdf_t * domain_t\n",
    "    t_ES[i - start_test] = integrate.trapz(to_integral, domain_t, dx = 0.01) / (1 - alpha_es)\n",
    "    \n",
    "    ind_var_one = int( hs_prices.size * (1-alpha_one))-1\n",
    "    ind_var_two = int( hs_prices.size * (1-alpha_two))-1\n",
    "    ind_es = int( hs_prices.size * (1-alpha_es))-1\n",
    "    HS_var_ones[i - start_test] = hs_prices.iloc[ind_var_one]\n",
    "    HS_var_twos[i - start_test] = hs_prices.iloc[ind_var_two]\n",
    "    HS_ES[i - start_test] = hs_prices.iloc[:ind_es+1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6b0440f9e8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "plt.figure()\n",
    "plt.plot(logR[-252 :].values, 'r')\n",
    "plt.plot(norm_var_ones, '-.b')\n",
    "plt.plot(norm_var_twos, '-.g')\n",
    "plt.plot(norm_ES, '-.m')\n",
    "plt.plot(t_var_ones, '--b')\n",
    "plt.plot(t_var_twos, '--g')\n",
    "plt.plot(t_ES, '--m')\n",
    "plt.plot(HS_var_ones, 'b')\n",
    "plt.plot(HS_var_twos, 'g')\n",
    "plt.plot(HS_ES, 'm')\n",
    "plt.title('risk measures estimated with all historical data')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('log return')\n",
    "plt.legend(['log return', 'normal var 95%', 'normal var 99%', 'normal ES 97.5%', 't var 95%', 't var 99%', 't ES 97.5%', 'HS 95%', 'HS 99%', 'HS ES' ], loc = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_data = my_data.resample('7d').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    231.000000\n",
       "mean       0.027505\n",
       "std        0.253305\n",
       "min       -0.586528\n",
       "25%       -0.071184\n",
       "50%       -0.008649\n",
       "75%        0.076894\n",
       "max        1.857460\n",
       "Name: Close, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "week_close = weekly_data['Close']\n",
    "week_logR = np.log(week_close).diff()\n",
    "week_logR.drop(week_logR.index[0], inplace = True)\n",
    "week_logR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_start_test = int(week_logR.count() - 52)\n",
    "week_norm_var_ones = np.zeros(52)\n",
    "week_norm_var_twos = np.zeros(52)\n",
    "week_norm_ES = np.zeros(52)\n",
    "week_t_var_ones = np.zeros(52)\n",
    "week_t_var_twos = np.zeros(52)\n",
    "week_t_ES = np.zeros(52)\n",
    "week_HS_var_ones = np.zeros(52)\n",
    "week_HS_var_twos = np.zeros(52)\n",
    "week_HS_ES = np.zeros(52)\n",
    "\n",
    "for i in range(week_start_test, week_logR.count()):\n",
    "    training_set = week_logR[: i]\n",
    "    x_ticks = np.linspace(min(training_set), max(abs(training_set)),2000)\n",
    "    hs_prices = training_set.sort_values()\n",
    "    negaR = training_set[training_set <= 0].dropna()\n",
    "    sortedNegaR = (-negaR).sort_values()\n",
    "    mirrorNegative = sortedNegaR.append(-sortedNegaR)\n",
    "    mu_log = np.mean(training_set)\n",
    "    sigma_log = np.std(training_set)\n",
    "    nega_t_para = stats.t.fit(training_set)\n",
    "    \n",
    "    week_norm_var_ones[i - week_start_test] = -(sigma_log * stats.norm.ppf(alpha_one) - mu_log)\n",
    "    week_norm_var_twos[i - week_start_test] = -(sigma_log * stats.norm.ppf(alpha_two) - mu_log)\n",
    "    week_norm_ES[i - week_start_test]  = -sigma_log * stats.norm.pdf(stats.norm.ppf(alpha_es))/(1-alpha_es)- mu_log\n",
    "    \n",
    "    week_t_var_ones[i - week_start_test] = -stats.t.ppf(alpha_one, df = nega_t_para[0] , loc = nega_t_para[1], scale = nega_t_para[2])\n",
    "    week_t_var_twos[i - week_start_test] = -stats.t.ppf(alpha_two, df = nega_t_para[0] , loc = nega_t_para[1], scale = nega_t_para[2])\n",
    "    \n",
    "    t_es_quantile = stats.t.ppf(alpha_es, df = nega_t_para[0] , loc = nega_t_para[1], scale = nega_t_para[2])\n",
    "    domain_t = x_ticks[x_ticks <= t_es_quantile]\n",
    "    pdf_t = stats.t.pdf(domain_t, df = nega_t_para[0] , loc = nega_t_para[1], scale = nega_t_para[2])\n",
    "    to_integral = pdf_t * domain_t\n",
    "    t_ES[i - week_start_test] = integrate.trapz(to_integral, domain_t, dx = 0.01) / (1 - alpha_es)\n",
    "    \n",
    "    week_ind_var_one = int( hs_prices.size * (1-alpha_one))-1\n",
    "    week_ind_var_two = int( hs_prices.size * (1-alpha_two))-1\n",
    "    week_ind_es = int( hs_prices.size * (1-alpha_es))-1\n",
    "    week_HS_var_ones[i - week_start_test] = hs_prices.iloc[ind_var_one]\n",
    "    week_HS_var_twos[i - week_start_test] = hs_prices.iloc[ind_var_two]\n",
    "    week_HS_ES[i - week_start_test] = hs_prices.iloc[:ind_es+1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7166277984540292, -0.010177199412833283, 0.09342069922797398)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nega_t_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6b046120f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(week_logR[-52 :].values, 'r')\n",
    "plt.plot(week_norm_var_ones, '-.b')\n",
    "plt.plot(week_norm_var_twos, '-.g')\n",
    "plt.plot(week_norm_ES, '-.m')\n",
    "plt.plot(week_t_var_ones, '--b')\n",
    "plt.plot(week_t_var_twos, '--g')\n",
    "plt.plot(week_t_ES, '--m')\n",
    "plt.plot(week_HS_var_ones, 'b')\n",
    "plt.plot(week_HS_var_twos, 'g')\n",
    "plt.plot(week_HS_ES, 'm')\n",
    "plt.title('risk measures estimated with all resampled historical data')\n",
    "plt.xlabel('time in week')\n",
    "plt.ylabel('log return')\n",
    "plt.legend(['log return', 'normal var 95%', 'normal var 99%', 'normal ES 97.5%', 't var 95%', 't var 99%', 't ES 97.5%', 'HS 95%', 'HS 99%', 'HS ES' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6b04678e10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbins = int(30) \n",
    "plt.figure()\n",
    "week_logR.hist(bins = numbins, normed=True)\n",
    "plt.plot(x_ticks, (stats.t.pdf(x_ticks, df = nega_t_para[0] , loc = nega_t_para[1], scale = nega_t_para[2])), '--b', alpha = 0.5)\n",
    "plt.plot(x_ticks, 1./((2.*np.pi)**0.5 *sigma_log)*np.exp(-((x_ticks - mu_log)/sigma_log)**2/2), 'r', alpha = 0.4)\n",
    "\n",
    "plt.title('Histogram of log-retunrs and Gaussian with the same mean and variance')\n",
    "plt.legend(['normal', 'histogram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resampled weekly and take the first day of the week as the observation.\n",
    "#sample size reduced to 231. After taking one year (52 week) out as test\n",
    "#set, the traning set is really small. Fitted with t and normal \n",
    "#distribution. Calculated var 95%, 99% and ES 97.5%. \n",
    "\n",
    "#In total, these \n",
    "#risk meansures perform worse than those estimated from the daily data.\n",
    "#mainly because of lacking training data, especally some extreme cases \n",
    "#might got lost during the resampling. The historical method seems suffer the most.\n",
    "\n",
    "#And among these, ES estimated from t distribution perform especially\n",
    "#badly. The reason for this is ES heavily relies on the tail of\n",
    "#distribution, not only quantile on the tail, but the shape of the tail.\n",
    "#All the losses in information from resampling, especially the extreme\n",
    "#observations, severely affects the shape of the estimated tail.\n",
    "\n",
    "#RippleParaResample  vs  RippleParaAll\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
